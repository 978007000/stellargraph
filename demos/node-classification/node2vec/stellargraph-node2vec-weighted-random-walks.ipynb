{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec with weighted random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook illustrates how `Node2Vec` method can be applied to learn low dimensional node embeddings of an edge weighted graph through *__weighted biased random walks__* over the graph. \n",
    "\n",
    "Specifically, the notebook demonstrates:\n",
    "1. Learning node embeddings using  [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) algorithm applied to a set of weighted biased random walks performed over a graph.\n",
    "2. Visualizing the node embeddings in 2-D using the [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm.\n",
    "3. Demonstrating how the node embeddings calculated using Word2Vec can be used as feature vectors in a downstream task such as node classification. \n",
    "4. Comparing node embeddings learnt from \"unweighted Node2Vec\" with \"weighted Node2Vec\" visually as well as in terms of accuracy of node classification task over the same underlying graph. \n",
    "5. Lastly, performing a quick check of equivalency of weighted biased random walks to unweighted biased random walks when all edge weights are identically 1.\n",
    "\n",
    "The example uses components from the `stellargraph`, `Gensim`, and `scikit-learn` libraries.\n",
    "\n",
    "**Note:** For clarity of exposition, this notebook forgoes the use of standard machine learning practices such as `Node2Vec` parameter tuning, node feature standarization, data splitting that handles class imbalance, classifier selection, and classifier tuning to maximize predictive accuracy. We leave such improvements to the reader.\n",
    "\n",
    "### References\n",
    "\n",
    "**1.** Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016. ([link](https://snap.stanford.edu/node2vec/))\n",
    "\n",
    "**2.** Distributed representations of words and phrases and their compositionality. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.  In Advances in Neural Information Processing Systems (NIPS), pp. 3111-3119, 2013. ([link](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf))\n",
    "\n",
    "**3.** Gensim: Topic modelling for humans. ([link](https://radimrehurek.com/gensim/))\n",
    "\n",
    "**4.** scikit-learn: Machine Learning in Python ([link](http://scikit-learn.org/stable/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/doc019/.envs/stellargraph/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/doc019/.envs/stellargraph/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings \n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "The dataset is the citation network Cora. It can be downloaded by clicking [here](https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz). The following is the description of the dataset from the publisher:\n",
    "\n",
    "> The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. The README file in the dataset provides more details. \n",
    "\n",
    "For this demo, we ignore the word vectors associated with each paper. We are only interested in the network structure and the **subject** attribute of each paper.\n",
    "\n",
    "Download and unzip the cora.tgz file to a location on your computer. \n",
    "\n",
    "We assume that the dataset is stored in the directory\n",
    "\n",
    "`~/data/cora/`\n",
    "\n",
    "where the files `cora.cites` and `cora.content` can be located.\n",
    "\n",
    "We are going to load the data into a networkx object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"~/data/cora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_location = os.path.expanduser(os.path.join(data_dir, \"cora.cites\"))\n",
    "g_nx_wt = nx.read_weighted_edgelist(path=cora_location)\n",
    "\n",
    "# load the node attribute data\n",
    "cora_data_location = os.path.expanduser(os.path.join(data_dir, \"cora.content\"))\n",
    "node_attr = pd.read_csv(cora_data_location, sep='\\t', header=None)\n",
    "values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows()}\n",
    "nx.set_node_attributes(g_nx_wt, values, 'subject')\n",
    "\n",
    "# Select the largest connected component. For clarity we ignore isolated\n",
    "# nodes and subgraphs; having these in the data does not prevent the\n",
    "# algorithm from running and producing valid results.\n",
    "g_nx_wt = max(nx.connected_component_subgraphs(g_nx_wt, copy=True), key=len)\n",
    "print(\"Largest subgraph statistics: {} nodes, {} edges\".format(\n",
    "    g_nx_wt.number_of_nodes(), g_nx_wt.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weights to the edges \n",
    "For weighted biased random walks the underlying graph should have weights over the edges. Since the links in the Cora dataset are unweighted, we need to synthetically add weights to the links in the graph. One possibility is to weight each edge by the similarity of its end nodes. Here we assign the jaccard similarity of the features of the pair of nodes as the weight of edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = node_attr.copy()\n",
    "df.set_index(0, inplace = True)\n",
    "papers = df.index\n",
    "## calculating the paiwise jaccard similarity between each pair of nodes.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    wts = pd.DataFrame(1- pairwise_distances(df.iloc[:,:-1].values, metric = 'jaccard') , index = papers, columns = papers)\n",
    "    wts.index = wts.index.map(str)\n",
    "    wts.columns = wts.columns.map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the weight attribute to the edges. Note, here we use the word 'weight' to label the weight value over the edge but it can be any other user specified label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u,v in g_nx_wt.edges():\n",
    "    val = wts[u][v]\n",
    "    g_nx_wt[u][v]['weight'] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick look at the weights of the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = list()\n",
    "for u,v in g_nx_wt.edges():\n",
    "    wts.append(g_nx_wt[u][v]['weight'])\n",
    "wts = sorted(wts, reverse = True)\n",
    "edgeCount = collections.Counter(wts)\n",
    "wt, cnt = zip(*edgeCount.items())\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(wt, cnt, width=0.005, color='b')\n",
    "plt.title(\"Edge weights histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"edge weights\")\n",
    "plt.xticks(np.linspace(0,1,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above distribution of edge weights illustrates that majority of linked nodes are insignificantly similar in terms of their attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Node2Vec algorithm\n",
    "\n",
    "The Node2Vec algorithm is a method for learning continuous feature respresentations for nodes in networks [1]. This approach can simply be described as a mapping of nodes to a low dimensional space of features that maximizes the likelihood of preservering neighborhood sgrtucture of the nodes. This approach is not tied to a fixed definition of neighborhood of a node but can be used in conjunction with different notions of node neighborhood, such as, homophily or structural equivalence, among other concepts.  The algorithm efficiently explores diverse neighborhoods of nodes through a biased random walk procedure that is parametrized to emulate a specific concept of the neighborhood of a node. \n",
    "\n",
    "Once a pre-defined number of walks, of fixed lengths, have been sampled, the low dimension embedding vectors of nodes can be learnt using `Word2vec` algorithm [2]. We use the Word2Vec implementation in the free Python library `Gensim` [3] to learn representations for each node in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus generation using random walks\n",
    "\n",
    "The stellargraph library provides an implementation of random walks that can be unweighted or weighted as required by Node2Vec. The random walks have a pre-defined fixed maximum length and are controlled by three parameters `p`, `q`,  and `weight`. By default, the weight over the edges is assumed to be 1. See [1] for a detailed description of these parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted random walks\n",
    "The first step for the weighted biased random walk is to build a random walk object by passing it a Stellargraph object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(StellarGraph(g_nx_wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to sample a set of random walks of pre-defined length starting from each node of the graph. Parameters `p`, `q`, and `weighted` influence the type of random walks in the procedure. In this demo, we are going to start 10 random walks from each node in the graph with a length up to 100. We set parameter `p` to 0.5 and `q` to 2.0 and the weight parameter set to True. The `run` method in the random walk will check if the weights over the edges are available and resolve other issues, such as, whether the weights are numeric and that their is no ambiguity of edge traversal (i.e. each pair of node is connected by a unique mnumerically weighted edge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_walks = rw.run(nodes=g_nx_wt.nodes(), # root nodes\n",
    "               length=100,  # maximum length of a random walk\n",
    "               n=10,        # number of random walks per root node \n",
    "               p=0.5,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "               q=2.0,        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "               weighted = True, #for weighted random walks\n",
    "               seed=42          # random seed fixed for reproducibility\n",
    "            )\n",
    "print(\"Number of random walks: {}\".format(len(weighted_walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation Learning using Word2Vec\n",
    "\n",
    "Once we have a sample set of walks, we learn the low-dimensional embedding of nodes using Word2Vec approach.\n",
    "We set the dimensionality of the learned embedding vectors to 128 as in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = Word2Vec(weighted_walks, size=128, window=5, min_count=0, sg=1, workers=1, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding vectors can be retrieved from model.wv using the node ID as key.\n",
    "# E.g., for node id '19231', the embedding vector is retrieved as\n",
    "emb = weighted_model.wv['19231']\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Node Embeddings generated by weighted  random walks\n",
    "\n",
    "We retrieve the `Word2Vec` node embeddings that are 128-dimensional vectors and then we project them down to 2 dimensions using the [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) algorithm for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = weighted_model.wv.index2word  # list of node IDs\n",
    "weighted_node_embeddings = weighted_model.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ g_nx_wt.node[node_id]['subject'] for node_id in node_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doc019/.envs/stellargraph/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py:71: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "  return matrix(data, dtype=dtype, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# Apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2 , random_state=42)\n",
    "weighted_node_embeddings_2d = tsne.fit_transform(weighted_node_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the embeddings generated from weighted random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the points\n",
    "alpha = 0.7\n",
    "label_map = {l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [ label_map[target] for target in node_targets]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(weighted_node_embeddings_2d[:,0], \n",
    "            weighted_node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap = \"jet\", alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream task\n",
    "\n",
    "The node embeddings calculated using `Word2Vec` can be used as feature vectors in a downstream task such as node classification. Here we give an example of training a logistic regression classifier using the node embeddings, learnt above, as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 128-dimensional input features\n",
    "X = weighted_node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test split\n",
    "We use 75% of the data for training and the remaining 25% for testing as a hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=None, random_state = 42\n",
    ")\n",
    "print(\"Array shapes:\\n X_train = {}\\n y_train = {}\\n X_test = {}\\n y_test = {}\" \\\n",
    "      .format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Training\n",
    "\n",
    "We train a Logistic Regression classifier on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=10, \n",
    "                           cv=10,\n",
    "                           tol=0.001,\n",
    "                           max_iter=1000,\n",
    "                           scoring=\"accuracy\",\n",
    "                           verbose=False,\n",
    "                           multi_class='ovr',\n",
    "                           random_state=5434)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the hold out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to weighted and unnweighted biased random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare weighted random walks with unweighted random walks. This simply requires toggling the weight parameter to False in the `run` method of the BiasedRandomWalk. Note, the weight parameter is by default set to False, hence, not specifying the weight parameter would result in the same action. \n",
    "\n",
    "#### Step 1: performing unweighted walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = rw.run(nodes=g_nx_wt.nodes(), # root nodes\n",
    "               length=100,  # maximum length of a random walk\n",
    "               n=10,        # number of random walks per root node \n",
    "               p=0.5,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "               q=2.0,        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "               weighted = False, # since we are interested in unweighted walks\n",
    "               seed=42           # for reproducibility\n",
    "              )\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: learning node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(walks, size=128, window=5, min_count=0, sg=1, workers=1, iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the embeddings generated from unweighted random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = model.wv.index2word  # list of node IDs\n",
    "node_embeddings = model.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ g_nx_wt.node[node_id]['subject'] for node_id in node_ids]\n",
    "\n",
    "# Apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "node_embeddings_2d = tsne.fit_transform(node_embeddings)\n",
    "\n",
    "# draw the points\n",
    "alpha = 0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [ label_map[target] for target in node_targets]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(node_embeddings_2d[:,0], \n",
    "            node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap = \"jet\", alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual comparison of node embedding plots for weighted and unweighted random walks illustrates the differences betweem the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using unweighted random walks to train classifiers to predict the subject of a paper in Cora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 128-dimensional input features\n",
    "X = node_embeddings  \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=None, random_state=42\n",
    ")\n",
    "print(\"Array shapes:\\n X_train = {}\\n y_train = {}\\n X_test = {}\\n y_test = {}\" \\\n",
    "      .format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=10, \n",
    "                           cv=10, \n",
    "                           tol=0.01,\n",
    "                           max_iter=1000,\n",
    "                           scoring=\"accuracy\",\n",
    "                           verbose=False,\n",
    "                           multi_class='ovr',\n",
    "                           random_state=5434)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the node embeddings extracted from unweighted random walks are more representative of the underlying community structure of the `Cora` dataset than the embeddings learnt from weighted random walks over the artificially weighted Cora network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing whether weights = 1 gives identical result to unweighted randomwalks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we perform a quick check of whether weighted biased random walks are identical to unweighted biased random walks when weights over the edges are identically 1. \n",
    "\n",
    "First, set weights of all edges in the graph to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u,v in g_nx_wt.edges():\n",
    "    g_nx_wt[u][v]['weight'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check to confirm if all edge weights are actually 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = list()\n",
    "for u,v in g_nx_wt.edges():\n",
    "    wts.append(g_nx_wt[u][v]['weight'])\n",
    "wts = sorted(wts, reverse = True)\n",
    "edgeCount = collections.Counter(wts)\n",
    "wt, cnt = zip(*edgeCount.items())\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar(wt, cnt, width=0.005, color='b')\n",
    "plt.title(\"Edge weights histogram\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"edge weights\")\n",
    "plt.xticks(np.linspace(0,1,10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: create the biased random walk object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(StellarGraph(g_nx_wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: sample random walks from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_walks = rw.run(nodes=g_nx_wt.nodes(), # root nodes\n",
    "               length=100,  # maximum length of a random walk\n",
    "               n=10,        # number of random walks per root node \n",
    "               p=0.5,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "               q=2.0,        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "               weighted = True, # indicates the walks are weighted\n",
    "               seed=42,         # seed fixed for reproducibility\n",
    "              )\n",
    "print(\"Number of random walks: {}\".format(len(weighted_walks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare unweighted walks with weighted walks when all weights are uniformly set to 1. Note, the two sets should be identical given all other parameters and random seeds are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert walks == weighted_walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: learn node embeddings from the walks sampled above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = Word2Vec(weighted_walks, size=128, window=5, min_count=0, sg=1, workers=1, iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the embeddings in 2-D using t-SNE tranformation for visual comparison to the embeddings learnt from unweighted walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings and corresponding subjects\n",
    "node_ids = weighted_model.wv.index2word  # list of node IDs\n",
    "weighted_node_embeddings = weighted_model.wv.vectors  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "node_targets = [ g_nx_wt.node[node_id]['subject'] for node_id in node_ids]\n",
    "\n",
    "# Apply t-SNE transformation on node embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "weighted_node_embeddings_2d = tsne.fit_transform(weighted_node_embeddings)\n",
    "\n",
    "# draw the points\n",
    "alpha = 0.7\n",
    "label_map = { l: i for i, l in enumerate(np.unique(node_targets))}\n",
    "node_colours = [ label_map[target] for target in node_targets]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(weighted_node_embeddings_2d[:,0], \n",
    "            weighted_node_embeddings_2d[:,1], \n",
    "            c=node_colours, cmap = \"jet\", alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the accuracy of node classification for weighted (weight ==1) and unweighted random walks.\n",
    "\n",
    "Compare classification of nodes through logistic regression on embeddings learnt from weighted (weight == 1) walks to that of unweighted walks demonstrated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will hold the 128-dimensional input features\n",
    "X = weighted_node_embeddings \n",
    "# y holds the corresponding target values\n",
    "y = np.array(node_targets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=None, random_state=42\n",
    ")\n",
    "print(\"Array shapes:\\n X_train = {}\\n y_train = {}\\n X_test = {}\\n y_test = {}\" \\\n",
    "      .format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=10, \n",
    "                           cv=10, \n",
    "                           tol=0.001,\n",
    "                           max_iter=1000,\n",
    "                           scoring=\"accuracy\",\n",
    "                           verbose=False,\n",
    "                           multi_class='ovr',\n",
    "                           random_state=5434)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(weighted_node_embeddings, node_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted random walks with weight == 1 are identical to unweighted random walks. Moreover, the embeddings learnt over the two kinds of walks are identical as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: \n",
    "\n",
    "The above demo of application of Node2Vec method on the CORA dataset using weighted biased random walks demonstrates, weighted biased random walks produce inherently different node embeddings from the embeddings learnt through unweighted random walks over the same graph, as illustrated by t-SNE visualization of the two as well as comparison of performance over node classification.  These differences illustrate that (realistic) weights on the edges of a graph can be leveraged to learn more accurate representation of nodes in low dimensional feature space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Twitter users who use hateful lexicon using graph machine learning with Stellargraph\n",
    "\n",
    "We consider the use-case of identifying hateful users on Twitter motivated by the work in [1] and using the dataset also published in [1]. Classification is based on a graph based on users' retweets and attributes as related to their account activity, and the content of tweets.\n",
    "\n",
    "We pose identifying hateful users as a binary classification problem and demonstrate the advantage of connected vs unconnected data with regards to increased prediction accuracy for a highly unbalanced dataset in a semi-supervised setting with few training examples.\n",
    "\n",
    "For connected data, we use Graph Neural Network methods, GCN [2], GAT [3], and GraphSAGE [4] as implemented in the `stellargraph` library. We pose the problem of idnetifying hateful tweeter users as node attribute inference in graph machine learning.\n",
    "\n",
    "**References**\n",
    "\n",
    "1. \"Like Sheep Among Wolves\": Characterizing Hateful Users on Twitter. M. H. Ribeiro, P. H. Calais, Y. A. Santos, V. A. F. Almeida, and W. Meira Jr.  arXiv preprint arXiv:1801.00317 (2017).\n",
    "\n",
    "\n",
    "2. Semi-Supervised Classification with Graph Convolutional Networks. T. Kipf, M. Welling. ICLR 2017. arXiv:1609.02907 \n",
    "\n",
    "\n",
    "3. Graph Attention Networks. P. Velickovic et al. ICLR 2018\n",
    "\n",
    "\n",
    "4. Inductive Representation Learning on Large Graphs. W.L. Hamilton, R. Ying, and J. Leskovec arXiv:1706.02216 \n",
    "[cs.SI], 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import GraphSAGENodeGenerator, FullBatchNodeGenerator\n",
    "from stellargraph.layer import GraphSAGE, GCN, GAT\n",
    "from stellargraph import globalvar\n",
    "\n",
    "from keras import layers, optimizers, losses, metrics, Model, models\n",
    "from sklearn import preprocessing, feature_extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_prefix(text, prefix):\n",
    "    return text[text.startswith(prefix) and len(prefix):]\n",
    "\n",
    "def plot_history(history):\n",
    "    metrics = sorted(set([remove_prefix(m, \"val_\") for m in list(history.history.keys())]))\n",
    "    for m in metrics:\n",
    "        # summarize history for metric m\n",
    "        plt.plot(history.history[m])\n",
    "        plt.plot(history.history['val_' + m])\n",
    "        plt.title(m, fontsize=18)\n",
    "        plt.ylabel(m, fontsize=18)\n",
    "        plt.xlabel('epoch', fontsize=18)\n",
    "        plt.legend(['train', 'validation'], loc='best')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading the dataset:**\n",
    "\n",
    "The dataset for this demo was published in [1] and it is freely available to download from Kaggle [here](https://www.kaggle.com/manoelribeiro/hateful-users-on-twitter/home).\n",
    "\n",
    "The following is the description of the datasets:\n",
    "\n",
    ">This dataset contains a network of 100k users, out of which ~5k were annotated as hateful or\n",
    ">not. For each user, several content-related, network-related and activity related features\n",
    ">were provided. \n",
    "\n",
    "Additional files of hateful lexicon can be found [here]( \n",
    "https://github.com/manoelhortaribeiro/HatefulUsersTwitter/tree/master/data/extra)\n",
    "\n",
    "Download the dataset and then set the `data_dir` variable to point to the download location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/data/hateful-twitter-users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load and prepare the node features\n",
    "\n",
    "Each node in the graph is associated with a large number of features. These are,\n",
    "\n",
    "hate :(\"hateful\"|\"normal\"|\"other\")\n",
    "  if user was annotated as hateful, normal, or not annotated.\n",
    "  \n",
    "  (is_50|is_50_2) :bool\n",
    "  whether user was deleted up to 12/12/17 or 14/01/18. \n",
    "  \n",
    "  (is_63|is_63_2) :bool\n",
    "  whether user was suspended up to 12/12/17 or 14/01/18. \n",
    "        \n",
    "  (hate|normal)_neigh :bool\n",
    "  is the user on the neighborhood of a (hateful|normal) user? \n",
    "  \n",
    "  [c_] (statuses|follower|followees|favorites)_count :int\n",
    "  number of (tweets|follower|followees|favorites) a user has.\n",
    "  \n",
    "  [c_] listed_count:int\n",
    "  number of lists a user is in.\n",
    "\n",
    "  [c_] (betweenness|eigenvector|in_degree|outdegree) :float\n",
    "  centrality measurements for each user in the retweet graph.\n",
    "  \n",
    "  [c_] *_empath :float\n",
    "  occurrences of empath categories in the users latest 200 tweets.\n",
    "\n",
    "  [c_] *_glove :float          \n",
    "  glove vector calculated for users latest 200 tweets.\n",
    "  \n",
    "  [c_] (sentiment|subjectivity) :float\n",
    "  average sentiment and subjectivity of users tweets.\n",
    "  \n",
    "  [c_] (time_diff|time_diff_median) :float\n",
    "  average and median time difference between tweets.\n",
    "  \n",
    "  [c_] (tweet|retweet|quote) number :float\n",
    "  percentage of direct tweets, retweets and quotes of an user.\n",
    "  \n",
    "  [c_] (number urls|number hashtags|baddies|mentions) :float\n",
    "  number of bad words|mentions|urls|hashtags per tweet in average.\n",
    "  \n",
    "  [c_] status length :float\n",
    "  average status length.\n",
    "  \n",
    "  hashtags :string\n",
    "  all hashtags employed by the user separated by spaces.\n",
    "  \n",
    "**Notice** that c_ are attributes calculated for the 1-neighborhood of a user in the retweet network (averaged out)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to load the user features and prepare them for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_feat = pd.read_csv(os.path.join(data_dir, \n",
    "                                      'users_neighborhood_anon.csv'))\n",
    "users_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the distribution of hateful, normal (not hateful), and other (unknown) users in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial hateful/normal users distribution\")\n",
    "print(users_feat.shape)\n",
    "print(users_feat.hate.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear imbalance on the number of users tagged as hateful vs normal and unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset as given includes a large number of graph related features that are manually extracted. \n",
    "\n",
    "Since we are going to emply modern graph neural networks methods for classification, we are going to drop these manually engineered features. \n",
    "\n",
    "The power of Graph Neural Networks stems from their ability to learn useful graph-related features eliminating the need for manual feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(feat):\n",
    "    feat = feat.drop(columns=[\"hate_neigh\", \"normal_neigh\"])\n",
    "    \n",
    "    # Convert target values in hate column from strings to integers (0,1,2)\n",
    "    feat['hate'] = np.where(feat['hate']=='hateful', 1, np.where(feat['hate']=='normal', 0, 2))\n",
    "    \n",
    "    # missing information\n",
    "    number_of_missing = feat.isnull().sum()\n",
    "    number_of_missing[number_of_missing!=0]\n",
    "    \n",
    "    # Replace NA with 0\n",
    "    feat.fillna(0, inplace=True)\n",
    "\n",
    "    # droping info about suspension and deletion as it is should not be use din the predictive model\n",
    "    feat.drop(feat.columns[feat.columns.str.contains(\"is_\")], axis=1, inplace=True)\n",
    "\n",
    "    # drop glove features\n",
    "    feat.drop(feat.columns[feat.columns.str.contains(\"_glove\")], axis=1, inplace=True)\n",
    "\n",
    "    # drop c_ features\n",
    "    feat.drop(feat.columns[feat.columns.str.contains(\"c_\")], axis=1, inplace=True)\n",
    "\n",
    "    # drop sentiment features for now\n",
    "    feat.drop(feat.columns[feat.columns.str.contains(\"sentiment\")], axis=1, inplace=True)\n",
    "\n",
    "    # drop hashtag feature\n",
    "    feat.drop(['hashtags'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop centrality based measures\n",
    "    feat.drop(columns=['betweenness', 'eigenvector', 'in_degree', 'out_degree'], inplace=True)\n",
    "    \n",
    "    feat.drop(columns=['created_at'], inplace=True)\n",
    "    \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = data_cleaning(users_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the original **1037** node features, we are keeping only **204** that are based on a user's attributes and tweet lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next apply normalization for the continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = node_data.columns[1:].values # contains everything excluding user_id\n",
    "len(columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the first two columns because those are user_id and hate (the target variable)\n",
    "df_values = node_data.iloc[:, 2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = preprocessing.PowerTransformer(method='yeo-johnson', \n",
    "                                    standardize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values_log = pt.fit_transform(df_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at one of the normalized features before and after the power transform was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_rc = {'lines.linewidth': 3}                  \n",
    "sns.set_context(\"paper\", rc = paper_rc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_values[100, :])\n",
    "s = plt.ylabel(\"Density\", fontsize=18)\n",
    "s = plt.xlabel(\"Feature value\", fontsize=18)\n",
    "s = plt.title(\"Before Power Transform\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_values_log[100, :])\n",
    "s = plt.ylabel(\"Density\", fontsize=18)\n",
    "s = plt.xlabel(\"Feature value\", fontsize=18)\n",
    "s = plt.title(\"After Power Transform\", fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization looks like it is doing the right thing as the raw features have long tails that are eliminated after applying the power transform. \n",
    "\n",
    "So let us use the normalized features from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.iloc[:, 2:] = df_values_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataframe index to be the same as the user_id and drop the user_id columns\n",
    "node_data.index = node_data.index.map(str)\n",
    "node_data.drop(columns=['user_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node features are now ready for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next load the graph\n",
    "\n",
    "Now that we have the node features prepared for machine learning, let us load the retweet graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nx = nx.read_edgelist(path=os.path.expanduser(os.path.join(data_dir,\n",
    "                                                             \"users.edges\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nx.number_of_nodes(), g_nx.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph has just over 100k nodes and approximately 2.2m edges.\n",
    "\n",
    "We aim to train a graph neural network model that will predict the \"hate\"attribute on the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(node_data[\"hate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning we want to take a subset of the nodes for training, and use the rest for validation and testing. We'll use scikit-learn again to split our data into training and test sets.\n",
    "\n",
    "The total number of annotated nodes is very small when compared to the totail number of nodes in the graph. We are only going to use 15% of the annotated nodes for training and the reamining 85% of nodes for testing.\n",
    "\n",
    "First, we are going to select the subset of nodes that are annotated as hateful or normal. These will be the nodes that have 'hate' values that are either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the nodes annotated with normal or hateful classes\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "annotated_users = node_data[node_data['hate']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_user_features = annotated_users.drop(columns=['hate'])\n",
    "annotated_user_targets = annotated_users['hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4971 annoted nodes out of a possible, approximately, 100k nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotated_user_targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "train_data, test_data, train_targets, test_targets = train_test_split(annotated_user_features,\n",
    "                                         annotated_user_targets,\n",
    "                                         test_size=0.85,\n",
    "                                         random_state=101)\n",
    "train_targets = train_targets.values\n",
    "train_targets = train_targets[...,np.newaxis]\n",
    "test_targets = test_targets.values\n",
    "test_targets = test_targets[...,np.newaxis]\n",
    "#train_data.drop(columns=['hate'], inplace=True)\n",
    "#test_data.drop(columns=['hate'], inplace=True)\n",
    "print(\"Sizes and class distributions for train/test data\")\n",
    "print(\"Shape train_data {}\".format(train_data.shape))\n",
    "print(\"Shape test_data {}\".format(test_data.shape))\n",
    "print(\"Train data number of 0s {} and 1s {}\".format(np.sum(train_targets==0), \n",
    "                                                    np.sum(train_targets==1)))\n",
    "print(\"Test data number of 0s {} and 1s {}\".format(np.sum(test_targets==0), \n",
    "                                                   np.sum(test_targets==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.shape, test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 745 nodes for training and 4226 nodes for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing features to assign to a graph, excluding target variable\n",
    "node_features = node_data.drop(columns=['hate'])\n",
    "node_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with imbalanced data\n",
    "\n",
    "As the model is imblanaced we introduce class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', \n",
    "                                     np.unique(train_targets), \n",
    "                                     train_targets[:,0])\n",
    "train_class_weights = dict(zip(np.unique(train_targets), \n",
    "                               class_weights))\n",
    "train_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for machine learning.\n",
    "\n",
    "Node features are stored in the Pandas DataFrame `node_features`.\n",
    "\n",
    "The graph in networkx format is stored in the variable `g_nx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify global parameters\n",
    "\n",
    "Here we specify some parameters that control the type of model we are going to use. For example, we specify the base model type, e.g., GCN, GraphSAGE, etc, and the number of estimators in the ensemble as well as model-specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'graphsage'    # Can be either gcn, gat, or graphsage\n",
    "\n",
    "if model_type == \"graphsage\":\n",
    "    # For GraphSAGE model\n",
    "    batch_size = 50; \n",
    "    num_samples = [20, 10]\n",
    "    epochs = 30          # The number of training epochs\n",
    "elif model_type == \"gcn\":\n",
    "    # For GCN model\n",
    "    epochs = 20          # The number of training epochs\n",
    "elif model_type == \"gat\":\n",
    "    # For GAT model\n",
    "    layer_sizes = [8, 1]\n",
    "    attention_heads = 8\n",
    "    epochs = 20         # The number of training epochs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the base graph machine learning model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a `StellarGraph` object from the `NetworkX` graph and the node features and targets. It is `StellarGraph` objects that we use in this library to perform machine learning tasks on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = sg.StellarGraph(g_nx, node_features=node_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed data from the graph to the Keras model we need a generator. The generators are specialized to the model and the learning task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we map only the training nodes returned from our splitter and the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'graphsage':\n",
    "    generator = GraphSAGENodeGenerator(G, batch_size, num_samples)\n",
    "    train_gen = generator.flow(train_data.index, \n",
    "                               train_targets, \n",
    "                               shuffle=True)\n",
    "elif model_type == 'gcn': \n",
    "    generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
    "    train_gen = generator.flow(train_data.index, \n",
    "                               train_targets, )\n",
    "elif model_type == 'gat':\n",
    "    generator = FullBatchNodeGenerator(G, method=\"gat\", sparse=True)\n",
    "    train_gen = generator.flow(train_data.index, \n",
    "                               train_targets,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify our machine learning model, we need a few more parameters for this but the parameters are model-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'graphsage':\n",
    "    base_model = GraphSAGE(\n",
    "        layer_sizes=[32, 32],\n",
    "        generator=train_gen,\n",
    "        bias=True,\n",
    "        dropout=0.5,\n",
    "    )\n",
    "    x_inp, x_out = base_model.default_model(flatten_output=True)\n",
    "    prediction = layers.Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "elif model_type == 'gcn':\n",
    "    base_model = GCN(\n",
    "        layer_sizes=[32, 16],\n",
    "        generator = generator,\n",
    "        bias=True,\n",
    "        dropout=0.5,\n",
    "        activations=[\"elu\", \"elu\"]\n",
    "    )\n",
    "    x_inp, x_out = base_model.node_model()\n",
    "    prediction = layers.Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "elif model_type == 'gat':\n",
    "    base_model = GAT(\n",
    "        layer_sizes=layer_sizes,\n",
    "        attn_heads=attention_heads,\n",
    "        generator=generator,\n",
    "        bias=True,\n",
    "        in_dropout=0.5,\n",
    "        attn_dropout=0.5,\n",
    "        activations=[\"elu\", \"sigmoid\"],\n",
    "        normalize=None,\n",
    "    )\n",
    "    x_inp, prediction = base_model.node_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the actual Keras model with the graph inputs `x_inp` provided by the `base_model` and outputs being the predictions from the softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_inp, outputs=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile our Keras model to use the `Adam` optimiser and the binary cross entroy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.005),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is of type stellargraph.utils.ensemble.Ensemble but has \n",
    "# a very similar interface to a Keras model\n",
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, keeping track of its loss and accuracy on the training set, and its performance on the validation set during the training (e.g., for early stopping), and generalization performance of the final model on a held-out test set (we need to create another generator over the test data for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator.flow(test_data.index, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model by calling the `fit_generator` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "if model_type == 'graphsage':\n",
    "    class_weight=train_class_weights\n",
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_gen,\n",
    "    verbose=0,\n",
    "    shuffle=False,\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the model, let's evaluate it on the test set.\n",
    "\n",
    "We are going to consider 4 evaluation metrics calculated on the test set: Accuracy, Area Under the ROC curve (AU-ROC), the ROC curve, and the confusion table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate_generator(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AU-ROC\n",
    "\n",
    "Let's use the trained GNN model to make a prediction for each node in the graph.\n",
    "\n",
    "Then, select only the predictions for the nodes in the test set and calculate the AU-ROC as another performance metric in addition to the accuracy shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = node_data.index\n",
    "all_gen = generator.flow(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = model.predict_generator(all_gen).squeeze()[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_df = pd.DataFrame(all_predictions, \n",
    "                                  index=node_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the predictions for the test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = all_predictions_df.loc[test_data.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are the probability of the true class that in this case is the probability of a user being hateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = test_preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_class = ((test_predictions>0.5)*1).flatten()\n",
    "test_df = pd.DataFrame({\"Predicted_score\": test_predictions.flatten(), \n",
    "                        \"Predicted_class\": test_predictions_class, \n",
    "                        \"True\": test_targets[:,0]})\n",
    "roc_auc = metrics.roc_auc_score(test_df['True'].values, \n",
    "                                test_df['Predicted_score'].values)\n",
    "print(\"The AUC on test set:\\n\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test_df['True'], test_df['Predicted_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(test_df['True'], test_df['Predicted_score'], pos_label=1)\n",
    "plt.figure(figsize=(12,6,))\n",
    "\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkblue',\n",
    "         lw=lw, label='GNN ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic curve', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of node embeddings\n",
    "\n",
    "Evaluate node embeddings as activations of the output of one of the graph convolutional or aggregation layers in the Keras model, and visualise them, coloring nodes by their subject label.\n",
    "\n",
    "You can find the index of the layer of interest by calling `model.layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a Keras model for calculating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'graphsage':\n",
    "    # For GraphSAGE, we are going to use the output activations \n",
    "    # of the second GraphSAGE layer as the node embeddings\n",
    "    # x_inp, prediction\n",
    "    emb_model = Model(inputs=x_inp, outputs=model.layers[-4].output)\n",
    "    emb = emb_model.predict_generator(generator=all_gen, )\n",
    "elif model_type == 'gcn':\n",
    "    # For GCN, we are going to use the output activations of \n",
    "    # the second GCN layer as the node embeddings\n",
    "    emb_model = Model(inputs=x_inp, outputs=model.layers[6].output)\n",
    "    emb = emb_model.predict_generator(generator=all_gen)\n",
    "elif model_type == 'gat':\n",
    "    # For GAT, we are going to use the output activations of the \n",
    "    # first Graph Attention layer as the node embeddings\n",
    "    emb_model = Model(inputs=x_inp, outputs=model.layers[6].output)\n",
    "    emb = emb_model.predict_generator(generator=all_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"graphsage\":\n",
    "    emb_all_df = pd.DataFrame(emb, index=node_data.index)\n",
    "elif model_type == \"gcn\" or model_type == \"gat\":\n",
    "    emb_all_df = pd.DataFrame(emb, index=G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the embeddings for the test set. We are only going to visualise the test set embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_test = emb_all_df.loc[test_data.index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the embeddings to 2d using either TSNE or PCA transform, and visualise, coloring nodes by their subject label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emb_test\n",
    "y = test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = TSNE # or use PCA \n",
    "\n",
    "trans = transform(n_components=2)\n",
    "emb_transformed = pd.DataFrame(trans.fit_transform(X), index=test_data.index)\n",
    "emb_transformed['label'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.scatter(emb_transformed[0], emb_transformed[1], c=emb_transformed['label'].astype(\"category\"), \n",
    "            cmap=\"jet\", alpha=alpha)\n",
    "ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "plt.title('{} visualization of embeddings for tweeter dataset'.format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node embeddings shown above indicate that the majority of hateful users tend to cluster together. However, some normal users are also in the same neighbourhood and these will be difficult to distinguish from hateful ones. Similarly, there is a small number of hateful users dispersed among normal users and these will also be difficult classify correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using Logistic Regression\n",
    "\n",
    "Finally, we train a Logistic Regression model on the same train and test data but this time ignoring the graph structure and focusing entirely on the node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `train_data`, `test_data`, `train_targets`, and `test_targets`, hold the data we need to train the Logistic Regression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, \n",
    "                          class_weight=class_weight, \n",
    "                          max_iter=10000)  # Let's use the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(train_data, train_targets.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the trained model to predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lr = lr.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate AUC metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_class_lr = ((test_preds_lr[:, 1]>0.5)*1).flatten()\n",
    "test_df_lr = pd.DataFrame({\"Predicted_score\": test_preds_lr[:, 1].flatten(), \n",
    "                        \"Predicted_class\": test_predictions_class_lr, \n",
    "                        \"True\": test_targets[:,0]})\n",
    "roc_auc_lr = metrics.roc_auc_score(test_df_lr['True'].values, test_df_lr['Predicted_score'].values)\n",
    "print(\"The AUC on test set:\\n\")\n",
    "print(roc_auc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test_df_lr['True'], test_df_lr['Predicted_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(test_df_lr['True'], test_df_lr['Predicted_score'], pos_label=1)\n",
    "plt.figure(figsize=(12,6,))\n",
    "lw = 2\n",
    "plt.plot(fpr_lr, tpr_lr, color='darkorange',\n",
    "         lw=lw, label='LR ROC curve (area = %0.2f)' % roc_auc_lr)\n",
    "plt.plot(fpr, tpr, color='darkblue',\n",
    "         lw=lw, label='GNN ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic curve', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at the True Positive and False Positive Rates for the GNN and Logistic Regression approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fp_rates = pd.DataFrame({\"tpr gnn\": tpr[0:100], \n",
    "                            \"fpr gnn\": fpr[0:100],\n",
    "                            \"tpr lr\": tpr_lr[0:100],\n",
    "                            \"fpr lr\": fpr_lr[0:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fp_rates.iloc[60:61, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparison between LR and GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note:** This comparison is valid when comparing GraphSAGE with Logistic Regression using a specific split of the data. Using a different GNN algorithm will very likely produce different numerical results, although, the conclusion below will still generally stand.\n",
    "\n",
    "Comparing the ROC curves between the two machine leanring methods, we see that adding the relatioship information in our machine learning model via the training of a GNN, improves overall performance.\n",
    "\n",
    "When classifying a user as hateful, it is important to minimise the number of false positives that is the number of normal users that are incorrectly classified as hateful. At the same time, we would like to classify as many hateful users as possible. We can achieve both of these goals by setting decision thresholds according guided by the ROC curve. \n",
    "\n",
    "The table above shows that if we are willing to tolerate approximately a false positive rate of approximately 1%, then the GNN model achieves a true positive rate of 32% whereas the logistic regression model achieves a true positive rate of 18%. Utilizing the information in the relationships between users improves true positive rate by 14% for approximately the same false positive rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
